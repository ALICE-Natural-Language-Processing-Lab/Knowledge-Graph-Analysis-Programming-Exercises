{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6-2\n",
    "\n",
    "# Entity-Relation-MLP (ER-MLP)\n",
    "\n",
    "In this exercise we will discuss another relational model based on multi layer perceptron(MLP). Similar to exercise 6-1 we will create composite representation of triple and will use non linear function to score the triple for link prediction.\n",
    "In E-MLP we define vector $W_k$ and $A_k = [a_i,a_j]$ for every relation, which requires $H_a + (H_a \\times 2 H_e)$ additional parameters for every relation. In our case H_e = H_k = k.  \n",
    "\n",
    "An alternative is to embed the relation into vector space and use it to create a composite representation of triple.\n",
    "In the following we define ER-MLP to model triples in a given knowledge graph $KG=<e_i,r_k, e_j>$:\n",
    "\n",
    "$f_{ijk}^{ER-MLP} = r^Tg(h_{ijk})$\n",
    "\n",
    "$h_{ijk} = W^Tx_{ij}^{ER-MLP}$\n",
    "\n",
    "$x_{ij}^{ER-MLP} = [a_i,a_j,a_k]$\n",
    "\n",
    "where g is a non linearity defined as element-wise operation $g(v_i)=tanh(v_i)$ over hidden representations $h_{ijk}$. Every element of hidden representation $h_{ijk}$ is obtained by applying linear transformation on composite representation $x_{ij}$. Each element in composite representation is obtained by stacking latent representation of triple: head, tail and relation $[a_i, a_j, a_k]$.\n",
    "\n",
    "$W$ is the hidden representation matrix. \n",
    "\n",
    "\n",
    "Each element of $f_{ijk}^{E-MLP}$ is a confidence of triple $<e_i,r_k, e_j>$. Similar to previous exercise we can formulate a link prediction problem using binary cross entropy loss function and solve it using gradient based methods:\n",
    "\n",
    "$L_{ijk} = x_{ijk} log \\sigma (f_{ijk}^{ER-MLP}) + (1-x_{ijk}) log \\sigma (f_{ijk}^{ER-MLP})$\n",
    "\n",
    "where $x_{ijk}=1$ if triple $<e_i,r_k,e_j>$ exists and $x_{ijk} = 0$ otherwise.\n",
    "\n",
    "For evaluations of method we will use the kinship dataset representing 26 relations (brother, sister, father,...} between 104 people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import get_minibatches, sample_negatives, accuracy, auc\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ERMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    ER-MLP: Entity-Relation MLP\n",
    "    ---------------------------\n",
    "    Dong, Xin, et al. \"Knowledge vault: A web-scale approach to probabilistic knowledge fusion.\" KDD, 2014.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, n_r, k, lam, gpu=False):\n",
    "        \"\"\"\n",
    "        ER-MLP: Entity-Relation MLP\n",
    "        ---------------------------\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "            n_e: int\n",
    "                Number of entities in dataset.\n",
    "\n",
    "            n_r: int\n",
    "                Number of relationships in dataset.\n",
    "\n",
    "            k: int\n",
    "                Embedding size.\n",
    "\n",
    "            lam: float\n",
    "                Prior strength of the embeddings. Used to constaint the\n",
    "                embedding norms inside a (euclidean) unit ball. The prior is\n",
    "                Gaussian, this param is the precision.\n",
    "\n",
    "            gpu: bool, default: False\n",
    "                Whether to use GPU or not.\n",
    "        \"\"\"\n",
    "        super(ERMLP, self).__init__()\n",
    "\n",
    "        # Hyperparams\n",
    "        self.n_e = n_e\n",
    "        self.n_r = n_r\n",
    "        self.k = k\n",
    "        self.lam = lam\n",
    "        self.gpu = gpu\n",
    "        # Nets\n",
    "        self.emb_E = nn.Embedding(self.n_e, self.k)\n",
    "        self.emb_R = nn.Embedding(self.n_r, self.k)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3*k, k),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(k, 1),\n",
    "        )\n",
    "\n",
    "        self.embeddings = [self.emb_E, self.emb_R]\n",
    "        self.initialize_embeddings()\n",
    "\n",
    "        # Xavier init\n",
    "        for p in self.mlp.modules():\n",
    "            if isinstance(p, nn.Linear):\n",
    "                in_dim = p.weight.size(0)\n",
    "                p.weight.data.normal_(0, 1/np.sqrt(in_dim/2))\n",
    "\n",
    "        # Copy all params to GPU if specified\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Decompose X into head, relationship, tail\n",
    "        hs, ls, ts = X[:, 0], X[:, 1], X[:, 2]\n",
    "\n",
    "        if self.gpu:\n",
    "            hs = Variable(torch.from_numpy(hs).cuda())\n",
    "            ls = Variable(torch.from_numpy(ls).cuda())\n",
    "            ts = Variable(torch.from_numpy(ts).cuda())\n",
    "        else:\n",
    "            hs = Variable(torch.from_numpy(hs))\n",
    "            ls = Variable(torch.from_numpy(ls))\n",
    "            ts = Variable(torch.from_numpy(ts))\n",
    "\n",
    "        # Project to embedding, each is M x k\n",
    "        e_hs = self.emb_E(hs)\n",
    "        e_ts = self.emb_E(ts)\n",
    "        e_ls = self.emb_R(ls)\n",
    "\n",
    "        # Forward\n",
    "        phi = torch.cat([e_hs, e_ts, e_ls], 1)  # M x 3k\n",
    "        y = self.mlp(phi)\n",
    "\n",
    "        return y.view(-1, 1)\n",
    "    \n",
    "    def predict(self, X, sigmoid=False):\n",
    "        \"\"\"\n",
    "        Predict the score of test batch.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        X: int matrix of M x 3, where M is the (mini)batch size\n",
    "            First row contains index of head entities.\n",
    "            Second row contains index of relationships.\n",
    "            Third row contains index of tail entities.\n",
    "\n",
    "        sigmoid: bool, default: False\n",
    "            Whether to apply sigmoid at the prediction or not. Useful if the\n",
    "            predicted result is scores/logits.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred: np.array of Mx1\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X).view(-1, 1)\n",
    "\n",
    "        if sigmoid:\n",
    "            y_pred = F.sigmoid(y_pred)\n",
    "\n",
    "        if self.gpu:\n",
    "            return y_pred.cpu().data.numpy()\n",
    "        else:\n",
    "            return y_pred.data.numpy()\n",
    "\n",
    "    def log_loss(self, y_pred, y_true, average=True):\n",
    "        \"\"\"\n",
    "        Compute log loss (Bernoulli NLL).\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pred: vector of size Mx1\n",
    "            Contains prediction logits.\n",
    "\n",
    "        y_true: np.array of size Mx1 (binary)\n",
    "            Contains the true labels.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        if self.gpu:\n",
    "            y_true = Variable(torch.from_numpy(y_true.astype(np.float32)).cuda())\n",
    "        else:\n",
    "            y_true = Variable(torch.from_numpy(y_true.astype(np.float32)))\n",
    "\n",
    "        nll = F.binary_cross_entropy_with_logits(y_pred, y_true, size_average=average)\n",
    "\n",
    "        norm_E = torch.norm(self.emb_E.weight, 2, 1)\n",
    "        norm_R = torch.norm(self.emb_R.weight, 2, 1)\n",
    "\n",
    "        # Penalize when embeddings norms larger than one\n",
    "        nlp1 = torch.sum(torch.clamp(norm_E - 1, min=0))\n",
    "        nlp2 = torch.sum(torch.clamp(norm_R - 1, min=0))\n",
    "\n",
    "        if average:\n",
    "            nlp1 /= nlp1.size(0)\n",
    "            nlp2 /= nlp2.size(0)\n",
    "\n",
    "        return nll + self.lam*nlp1 + self.lam*nlp2\n",
    "\n",
    "    def ranking_loss(self, y_pos, y_neg, margin=1, C=1, average=True):\n",
    "        \"\"\"\n",
    "        Compute loss max margin ranking loss.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pos: vector of size Mx1\n",
    "            Contains scores for positive samples.\n",
    "\n",
    "        y_neg: np.array of size M*Cx1 (binary)\n",
    "            Contains scores of negative samples or hard labels.\n",
    "\n",
    "        margin: float, default: 1\n",
    "            Margin used for the loss.\n",
    "\n",
    "        C: int, default: 1\n",
    "            Number of negative samples per positive sample.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        M = y_pos.size(0)\n",
    "\n",
    "        y_pos = y_pos.view(-1).repeat(C)  # repeat to match y_neg\n",
    "        y_neg = y_neg.view(-1)\n",
    "\n",
    "        # target = [-1, -1, ..., -1], i.e. y_neg should be higher than y_pos\n",
    "        target = -np.ones(M*C, dtype=np.float32)\n",
    "\n",
    "        if self.gpu:\n",
    "            target = Variable(torch.from_numpy(target).cuda())\n",
    "        else:\n",
    "            target = Variable(torch.from_numpy(target))\n",
    "\n",
    "        loss = F.margin_ranking_loss(\n",
    "            y_pos, y_neg, target, margin=margin, size_average=average\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def normalize_embeddings(self):\n",
    "        for e in self.embeddings:\n",
    "            e.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "\n",
    "    def initialize_embeddings(self):\n",
    "        r = 6/np.sqrt(self.k)\n",
    "\n",
    "        for e in self.embeddings:\n",
    "            e.weight.data.uniform_(-r, r)\n",
    "\n",
    "        self.normalize_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb54c6601b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "randseed = 9999\n",
    "np.random.seed(randseed)\n",
    "torch.manual_seed(randseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "# Load dictionary lookups\n",
    "idx2ent = np.load('data/kinship/bin/idx2ent.npy')\n",
    "idx2rel = np.load('data/kinship/bin/idx2rel.npy')\n",
    "\n",
    "n_e = len(idx2ent)\n",
    "n_r = len(idx2rel)\n",
    "\n",
    "# Load dataset\n",
    "X_train = np.load('data/kinship/bin/train.npy')\n",
    "X_val = np.load('data/kinship/bin/val.npy')\n",
    "y_val = np.load('data/kinship/bin/y_val.npy')\n",
    "\n",
    "X_val_pos = X_val[y_val.ravel() == 1, :]  # Take only positive samples\n",
    "\n",
    "M_train = X_train.shape[0]\n",
    "M_val = X_val.shape[0]\n",
    "\n",
    "# Model Parameters\n",
    "k = 50\n",
    "embeddings_lambda = 0\n",
    "model = ERMLP(n_e=n_e, n_r=n_r, k=k, lam=embeddings_lambda, gpu= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1\n",
      "----------------\n",
      "Iter-0; loss: 1.5382; train_auc: 0.4928; val_auc: 0.4991; val_loss: 1.3434; time per batch: 0.01s\n",
      "Epoch-2\n",
      "----------------\n",
      "Iter-0; loss: 0.7148; train_auc: 0.5125; val_auc: 0.5110; val_loss: 0.7071; time per batch: 0.00s\n",
      "Epoch-3\n",
      "----------------\n",
      "Iter-0; loss: 0.7142; train_auc: 0.5632; val_auc: 0.5086; val_loss: 0.7179; time per batch: 0.00s\n",
      "Epoch-4\n",
      "----------------\n",
      "Iter-0; loss: 0.7079; train_auc: 0.5151; val_auc: 0.5584; val_loss: 0.6992; time per batch: 0.00s\n",
      "Epoch-5\n",
      "----------------\n",
      "Iter-0; loss: 0.6708; train_auc: 0.6283; val_auc: 0.5990; val_loss: 0.6761; time per batch: 0.00s\n",
      "Epoch-6\n",
      "----------------\n",
      "Iter-0; loss: 0.6811; train_auc: 0.5962; val_auc: 0.6492; val_loss: 0.6470; time per batch: 0.00s\n",
      "Epoch-7\n",
      "----------------\n",
      "Iter-0; loss: 0.6668; train_auc: 0.6236; val_auc: 0.6691; val_loss: 0.6308; time per batch: 0.00s\n",
      "Epoch-8\n",
      "----------------\n",
      "Iter-0; loss: 0.6669; train_auc: 0.5917; val_auc: 0.6990; val_loss: 0.6153; time per batch: 0.00s\n",
      "Epoch-9\n",
      "----------------\n",
      "Iter-0; loss: 0.6447; train_auc: 0.6450; val_auc: 0.7105; val_loss: 0.6052; time per batch: 0.00s\n",
      "Epoch-10\n",
      "----------------\n",
      "Iter-0; loss: 0.6359; train_auc: 0.6905; val_auc: 0.7327; val_loss: 0.5838; time per batch: 0.00s\n",
      "Epoch-11\n",
      "----------------\n",
      "Iter-0; loss: 0.6660; train_auc: 0.6001; val_auc: 0.7330; val_loss: 0.5883; time per batch: 0.00s\n",
      "Epoch-12\n",
      "----------------\n",
      "Iter-0; loss: 0.6470; train_auc: 0.6531; val_auc: 0.7837; val_loss: 0.5545; time per batch: 0.00s\n",
      "Epoch-13\n",
      "----------------\n",
      "Iter-0; loss: 0.5729; train_auc: 0.7664; val_auc: 0.8566; val_loss: 0.5005; time per batch: 0.00s\n",
      "Epoch-14\n",
      "----------------\n",
      "Iter-0; loss: 0.4288; train_auc: 0.8768; val_auc: 0.8988; val_loss: 0.3996; time per batch: 0.01s\n",
      "Epoch-15\n",
      "----------------\n",
      "Iter-0; loss: 0.4441; train_auc: 0.8489; val_auc: 0.8850; val_loss: 0.3975; time per batch: 0.00s\n",
      "Epoch-16\n",
      "----------------\n",
      "Iter-0; loss: 0.4689; train_auc: 0.8464; val_auc: 0.9081; val_loss: 0.3646; time per batch: 0.00s\n",
      "Epoch-17\n",
      "----------------\n",
      "Iter-0; loss: 0.4744; train_auc: 0.8307; val_auc: 0.9254; val_loss: 0.3597; time per batch: 0.00s\n",
      "Epoch-18\n",
      "----------------\n",
      "Iter-0; loss: 0.3472; train_auc: 0.9153; val_auc: 0.9449; val_loss: 0.2942; time per batch: 0.00s\n",
      "Epoch-19\n",
      "----------------\n",
      "Iter-0; loss: 0.3357; train_auc: 0.8977; val_auc: 0.9536; val_loss: 0.2782; time per batch: 0.00s\n",
      "Epoch-20\n",
      "----------------\n",
      "Iter-0; loss: 0.3225; train_auc: 0.9181; val_auc: 0.9539; val_loss: 0.2671; time per batch: 0.00s\n"
     ]
    }
   ],
   "source": [
    "normalize_embed = True\n",
    "C = 10 # Negative Samples\n",
    "n_epoch = 20\n",
    "lr = 0.1\n",
    "lr_decay_every = 20\n",
    "#weight_decay = 1e-4\n",
    "mb_size = 100  \n",
    "print_every = 100\n",
    "average = True\n",
    "# Optimizer Initialization\n",
    "#solver = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "solver = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# Begin training\n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch-{}'.format(epoch+1))\n",
    "    print('----------------')\n",
    "    it = 0\n",
    "    # Shuffle and chunk data into minibatches\n",
    "    mb_iter = get_minibatches(X_train, mb_size, shuffle=True)\n",
    "\n",
    "    # Anneal learning rate\n",
    "    lr = lr * (0.5 ** (epoch // lr_decay_every))\n",
    "    for param_group in solver.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    for X_mb in mb_iter:\n",
    "        start = time()\n",
    "\n",
    "        # Build batch with negative sampling\n",
    "        m = X_mb.shape[0]\n",
    "        # C x M negative samples\n",
    "        X_neg_mb = sample_negatives(X_mb, n_e)\n",
    "        X_train_mb = np.vstack([X_mb, X_neg_mb])\n",
    "\n",
    "        y_true_mb = np.vstack([np.ones([m, 1]), np.zeros([m, 1])])\n",
    "\n",
    "        # Training step\n",
    "        y = model.forward(X_train_mb)\n",
    "        loss = model.log_loss(y, y_true_mb, average=average)\n",
    "        \n",
    "        loss.backward()\n",
    "        solver.step()\n",
    "        solver.zero_grad()\n",
    "        if normalize_embed:\n",
    "            model.normalize_embeddings()\n",
    "\n",
    "        end = time()\n",
    "        # Training logs\n",
    "        if it % print_every == 0:\n",
    "            # Training auc\n",
    "            pred = model.predict(X_train_mb, sigmoid=True)\n",
    "            train_acc = auc(pred, y_true_mb)\n",
    "            \n",
    "            # Per class accuracy\n",
    "            # pos_acc = accuracy(pred[:m], y_true_mb[:m])\n",
    "            # neg_acc = accuracy(pred[m:], y_true_mb[m:])\n",
    "\n",
    "            # Validation auc\n",
    "            y_pred_val = model.forward(X_val)\n",
    "            y_prob_val = F.sigmoid(y_pred_val)\n",
    "            \n",
    "            val_acc = auc(y_prob_val.data.numpy(), y_val)\n",
    "            # Validation loss\n",
    "            val_loss = model.log_loss(y_pred_val, y_val, average)\n",
    "\n",
    "            print('Iter-{}; loss: {:.4f}; train_auc: {:.4f}; val_auc: {:.4f}; val_loss: {:.4f}; time per batch: {:.2f}s'\n",
    "                    .format(it, loss.data[0], train_acc, val_acc, val_loss.data[0], end-start))\n",
    "\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1: Experiment with different evaluation metric\n",
    "\n",
    "In above implementation we use area under curve as an evaluation metric. In following part repeat above experiments using precision-recall-curve and accuracy. Which metric is more suitable for modeling link prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "###### Your Code Here\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2: Implement k-NN for Entity and Relation Set \n",
    "\n",
    "MLP based models are good in mapping semantically similar entities closer in vector space. Implement k-NN for entity and relations in knowledge graph. Code should work for different values of k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "###### Your Code Here\n",
    "###########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
